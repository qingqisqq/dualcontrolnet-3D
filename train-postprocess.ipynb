{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2ca7bf-bd3a-4c07-a5f0-51ccf58b4043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "project_dir = '/blue/shenhaowang/qingqisong/GenerativeUrbanDesign-main'\n",
    "os.chdir(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8a6da0-ef57-4cd8-b6b8-36cb6aa00a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cmake in /home/qingqisong/.local/lib/python3.11/site-packages (3.30.5)\n",
      "Requirement already satisfied: lit in /home/qingqisong/.local/lib/python3.11/site-packages (18.1.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6edafae-7ea5-4d41-b1cb-cfc44ba4293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio==3.16.2 \\\n",
    "albumentations==1.3.0 \\\n",
    "opencv-contrib-python \\\n",
    "imageio==2.9.0 \\\n",
    "imageio-ffmpeg==0.4.2 \\\n",
    "pytorch-lightning==1.5.0 \\\n",
    "omegaconf==2.1.1 \\\n",
    "test-tube>=0.7.5 \\\n",
    "streamlit==1.12.1 \\\n",
    "einops\\\n",
    "transformers \\\n",
    "webdataset==0.2.5 \\\n",
    "kornia==0.6 \\\n",
    "open_clip_torch==2.0.2 \\\n",
    "invisible-watermark>=0.1.5 \\\n",
    "streamlit-drawable-canvas==0.8.0 \\\n",
    "torchmetrics==0.6.0 \\\n",
    "timm==0.6.12 \\\n",
    "addict==2.4.0 \\\n",
    "yapf==0.32.0 \\\n",
    "prettytable==3.6.0 \\\n",
    "safetensors \\\n",
    "basicsr==1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e146ff8-162d-4294-8a67-1c5f0826431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: open_clip_torch in /home/qingqisong/.local/lib/python3.11/site-packages (2.0.2)\n",
      "Requirement already satisfied: torch>=1.9 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from open_clip_torch) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from open_clip_torch) (0.15.2)\n",
      "Requirement already satisfied: ftfy in /home/qingqisong/.local/lib/python3.11/site-packages (from open_clip_torch) (6.3.0)\n",
      "Requirement already satisfied: regex in /home/qingqisong/.local/lib/python3.11/site-packages (from open_clip_torch) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/qingqisong/.local/lib/python3.11/site-packages (from open_clip_torch) (4.66.5)\n",
      "Requirement already satisfied: huggingface-hub in /home/qingqisong/.local/lib/python3.11/site-packages (from open_clip_torch) (0.26.1)\n",
      "Requirement already satisfied: filelock in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torch>=1.9->open_clip_torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/qingqisong/.local/lib/python3.11/site-packages (from torch>=1.9->open_clip_torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torch>=1.9->open_clip_torch) (1.12)\n",
      "Requirement already satisfied: networkx in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torch>=1.9->open_clip_torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torch>=1.9->open_clip_torch) (3.1.3)\n",
      "Requirement already satisfied: wcwidth in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/qingqisong/.local/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (6.0.1)\n",
      "Requirement already satisfied: requests in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (2.31.0)\n",
      "Requirement already satisfied: numpy in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torchvision->open_clip_torch) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torchvision->open_clip_torch) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from jinja2->torch>=1.9->open_clip_torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from sympy->torch>=1.9->open_clip_torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "740a7fe5-0a76-4b6e-823b-ce6c8ac761ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: timm in /home/qingqisong/.local/lib/python3.11/site-packages (0.6.12)\n",
      "Collecting timm\n",
      "  Using cached timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: torch in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from timm) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from timm) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/qingqisong/.local/lib/python3.11/site-packages (from timm) (0.26.1)\n",
      "Requirement already satisfied: safetensors in /home/qingqisong/.local/lib/python3.11/site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/qingqisong/.local/lib/python3.11/site-packages (from huggingface_hub->timm) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from huggingface_hub->timm) (23.2)\n",
      "Requirement already satisfied: requests in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/qingqisong/.local/lib/python3.11/site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/qingqisong/.local/lib/python3.11/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: numpy in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /apps/pytorch/2.0.1b/lib/python3.11/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Using cached timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "Installing collected packages: timm\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 0.6.12\n",
      "    Uninstalling timm-0.6.12:\n",
      "      Successfully uninstalled timm-0.6.12\n",
      "Successfully installed timm-1.0.15\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37e29b6-308c-4b70-b134-eb5cb2ff6090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_dir, osm_dir, prompt_file, size=512):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        Args:\n",
    "            image_dir (str): Directory containing satellite images\n",
    "            osm_dir (str): Directory containing OSM maps\n",
    "            prompt_file (str): Path to the CSV file containing prompts\n",
    "            size (int): Target size for images (both height and width)\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.osm_dir = osm_dir\n",
    "        self.size = size\n",
    "        \n",
    "        # Read CSV file\n",
    "        self.df = pd.read_csv(prompt_file)\n",
    "        \n",
    "        # Extract required information\n",
    "        self.image_list_xtile = self.df['latitude'].tolist()\n",
    "        self.image_list_ytile = self.df['longitude'].tolist()\n",
    "        self.idx_list = self.df['idx'].tolist() if 'idx' in self.df.columns else list(range(len(self.df)))\n",
    "        self.prompts = self.df['prompt'].tolist() if 'prompt' in self.df.columns else [''] * len(self.df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list_xtile)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            xtile = self.image_list_xtile[idx]\n",
    "            ytile = self.image_list_ytile[idx]\n",
    "            item_idx = self.idx_list[idx]\n",
    "            \n",
    "            # Construct file paths\n",
    "            source_filename = os.path.join(self.osm_dir, f\"control_{item_idx}_{xtile}_{ytile}.png\")\n",
    "            target_filename = os.path.join(self.image_dir, f\"combined_{item_idx}_{xtile}_{ytile}.png\")\n",
    "            \n",
    "            # Read images\n",
    "            source = cv2.imread(source_filename)\n",
    "            target = cv2.imread(target_filename)\n",
    "            \n",
    "            if source is None:\n",
    "                raise FileNotFoundError(f\"Source image not found: {source_filename}\")\n",
    "            if target is None:\n",
    "                raise FileNotFoundError(f\"Target image not found: {target_filename}\")\n",
    "            \n",
    "            # Resize images to consistent size\n",
    "            source = cv2.resize(source, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "            target = cv2.resize(target, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            # Handle alpha channel in source image if present\n",
    "            if source.shape[2] == 4:\n",
    "                trans_mask = source[:, :, 3] == 0\n",
    "                source[trans_mask] = [255, 255, 255, 255]\n",
    "                source = cv2.cvtColor(source, cv2.COLOR_BGRA2BGR)\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
    "            target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Normalize\n",
    "            source = source.astype(np.float32) / 255.0  # [0, 1]\n",
    "            target = (target.astype(np.float32) / 127.5) - 1.0  # [-1, 1]\n",
    "            \n",
    "            return {\n",
    "                'jpg': target,\n",
    "                'txt': self.prompts[idx],\n",
    "                'hint': source\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading item {idx}: {str(e)}\")\n",
    "            return {\n",
    "                'jpg': np.zeros((self.size, self.size, 3), dtype=np.float32),\n",
    "                'txt': f\"Error: {str(e)}\",\n",
    "                'hint': np.zeros((self.size, self.size, 3), dtype=np.float32)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42208a-f1b0-474c-8388-2e4c0f6f1e12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qingqisong/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/qingqisong/.local/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n",
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loaded model config from [./models/cldm_v15.yaml]\n",
      "Loaded state_dict from [./lightning_logs/version_5227827/checkpoints/epoch=1-step=15275.ckpt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/qingqisong/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:118: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "/home/qingqisong/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:280: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/qingqisong/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper   | 859 M \n",
      "1 | first_stage_model | AutoencoderKL      | 83.7 M\n",
      "2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M \n",
      "3 | control_model     | ControlNet         | 361 M \n",
      "---------------------------------------------------------\n",
      "1.2 B     Trainable params\n",
      "206 M     Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,710.058 Total estimated model params size (MB)\n",
      "/home/qingqisong/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/7638 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qingqisong/.local/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "Epoch 0:   4%|▍         | 300/7638 [05:50<2:22:58,  1.17s/it, loss=0.106, v_num=5265894, train/loss_simple_step=0.132, train/loss_vlb_step=0.000606, train/consistency_loss_step=0.953, train/loss_step=0.133, global_step=299.0]   Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "Epoch 0:   8%|▊         | 600/7638 [11:40<2:16:56,  1.17s/it, loss=0.0929, v_num=5265894, train/loss_simple_step=0.0692, train/loss_vlb_step=0.000266, train/consistency_loss_step=0.549, train/loss_step=0.0697, global_step=599.0] Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "Epoch 0:  12%|█▏        | 900/7638 [17:30<2:11:05,  1.17s/it, loss=0.0914, v_num=5265894, train/loss_simple_step=0.0557, train/loss_vlb_step=0.000193, train/consistency_loss_step=0.479, train/loss_step=0.0562, global_step=899.0] Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "Epoch 0:  16%|█▌        | 1200/7638 [23:21<2:05:18,  1.17s/it, loss=0.109, v_num=5265894, train/loss_simple_step=0.209, train/loss_vlb_step=0.00329, train/consistency_loss_step=0.968, train/loss_step=0.210, global_step=1199.0]    "
     ]
    }
   ],
   "source": [
    "\n",
    "from share import *\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from cldm.logger import ImageLogger\n",
    "from cldm.model import create_model, load_state_dict\n",
    "\n",
    "\n",
    "dataset1_config = {\n",
    "    \"image_dir\": \"./combinedgrido150/\",\n",
    "    \"osm_dir\": \"./combinedcontrol_mapsgrido150/\",\n",
    "    \"prompt_file\": \"./metrics_datagrido150/dualtrainprompt.csv\"\n",
    "\n",
    "}\n",
    "\n",
    "dataset2_config = {\n",
    "    \"image_dir\": \"./combinedgridc180/\",\n",
    "    \"osm_dir\": \"./combinedcontrol_mapsgridc180/\",\n",
    "    \"prompt_file\": \"./metrics_datagridc180/dualtrainprompt.csv\"\n",
    "}\n",
    "\n",
    "# Configs\n",
    "#resume_path = './models/control_sd15_ini.ckpt'\n",
    "resume_path = './lightning_logs/version_5227827/checkpoints/epoch=1-step=15275.ckpt'\n",
    "\n",
    "batch_size = 4\n",
    "logger_freq = 300\n",
    "learning_rate = 1e-5\n",
    "sd_locked = True\n",
    "only_mid_control = False\n",
    "\n",
    "\n",
    "dataset1 = MyDataset(**dataset1_config)\n",
    "dataset2 = MyDataset(**dataset2_config)\n",
    "\n",
    "\n",
    "combined_dataset = ConcatDataset([dataset1, dataset2])\n",
    "\n",
    "\n",
    "dataloader = DataLoader(combined_dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model = create_model('./models/cldm_v15.yaml').cpu()\n",
    "model.load_state_dict(load_state_dict(resume_path, location='cpu'))\n",
    "model.learning_rate = learning_rate\n",
    "model.sd_locked = sd_locked\n",
    "model.only_mid_control = only_mid_control\n",
    "\n",
    "\n",
    "logger = ImageLogger(batch_frequency=logger_freq)\n",
    "trainer = pl.Trainer(max_epochs=3, gpus=1, precision=32, callbacks=[logger])\n",
    "\n",
    "\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4740fcf-d8fc-4e5d-b9c6-1df7af664ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define standard color dictionary for land use categories\n",
    "COLOR_MAPPING = {\n",
    "    'residential': (255, 202, 101),\n",
    "    'commercial': (255, 128, 128),\n",
    "    'industrial': (191, 191, 191),\n",
    "    'retail': (255, 85, 85),\n",
    "    'parking': (239, 239, 239),\n",
    "    'school': (255, 240, 170),\n",
    "    'university': (255, 240, 170),\n",
    "    'hospital': (255, 230, 230),\n",
    "    'park': (178, 216, 178),\n",
    "    'garden': (178, 216, 178),\n",
    "    'recreation_ground': (184, 230, 184),\n",
    "    'playground': (204, 230, 204),\n",
    "    'sports_centre': (204, 230, 204),\n",
    "    'stadium': (204, 230, 204),\n",
    "    'pitch': (204, 230, 204),\n",
    "    'golf_course': (178, 216, 178),\n",
    "    'forest': (140, 191, 140),\n",
    "    'wood': (140, 191, 140),\n",
    "    'grass': (204, 255, 204),\n",
    "    'grassland': (204, 255, 204),\n",
    "    'meadow': (204, 255, 204),\n",
    "    'heath': (204, 230, 204),\n",
    "    'scrub': (184, 230, 184),\n",
    "    'wetland': (186, 230, 230),\n",
    "    'water': (179, 217, 255),\n",
    "    'beach': (255, 245, 204),\n",
    "    'sand': (255, 245, 204),\n",
    "    'farmland': (255, 255, 204),\n",
    "    'orchard': (230, 255, 179),\n",
    "    'vineyard': (230, 255, 179),\n",
    "    'cemetery': (209, 207, 205),\n",
    "    'white': (255, 255, 255)  \n",
    "}\n",
    "\n",
    "def classify_pixel(pixel, color_mapping):\n",
    "    \"\"\"\n",
    "    Classify a pixel to the closest standard color using Euclidean distance\n",
    "    \n",
    "    Args:\n",
    "        pixel: Input pixel RGB values\n",
    "        color_mapping: Dictionary of standard colors\n",
    "    \n",
    "    Returns:\n",
    "        RGB values of the closest standard color\n",
    "    \"\"\"\n",
    "    min_distance = float('inf')\n",
    "    closest_category = None\n",
    "    \n",
    "    # Calculate distance to each standard color\n",
    "    for category, standard_color in color_mapping.items():\n",
    "        # Calculate Euclidean distance between pixel and standard color\n",
    "        distance = np.sqrt(sum((p - s) ** 2 for p, s in zip(pixel, standard_color)))\n",
    "        \n",
    "        # Update if this is the closest color so far\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_category = category\n",
    "    \n",
    "    return COLOR_MAPPING[closest_category]\n",
    "\n",
    "def convert_to_standard_colors(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert input land use image to standardized colors\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input image file\n",
    "        output_path: Path where output image will be saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read input image\n",
    "        img = Image.open(input_path)\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Create output array with same dimensions\n",
    "        output_array = np.zeros_like(img_array)\n",
    "        \n",
    "        # Process each pixel in the image\n",
    "        height, width = img_array.shape[:2]\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                pixel = img_array[y, x]\n",
    "                # Convert pixel to nearest standard color\n",
    "                standard_color = classify_pixel(pixel, COLOR_MAPPING)\n",
    "                output_array[y, x] = standard_color\n",
    "        \n",
    "        # Save the standardized image\n",
    "        output_img = Image.fromarray(output_array.astype('uint8'))\n",
    "        output_img.save(output_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_folder(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all osm_ images in the input folder\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Path to folder containing input images\n",
    "        output_folder: Path to folder where output images will be saved\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all osm_ images in input folder\n",
    "    osm_images = glob(os.path.join(input_folder, \"osm_*.png\"))\n",
    "    \n",
    "    # Process each image\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    print(f\"Found {len(osm_images)} images to process\")\n",
    "    \n",
    "    for input_path in tqdm(osm_images, desc=\"Processing images\"):\n",
    "        # Create output path\n",
    "        filename = os.path.basename(input_path)\n",
    "        output_path = os.path.join(output_folder, f\"standardized_{filename}\")\n",
    "        \n",
    "        # Process the image\n",
    "        if convert_to_standard_colors(input_path, output_path):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Successfully processed: {successful} images\")\n",
    "    print(f\"Failed to process: {failed} images\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"./m6fid_test_resultsextracted\"  \n",
    "output_folder = \"./standardizedm6fid\"  \n",
    "process_folder(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884d014-f86b-47da-adec-b700b56b3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def evaluate_land_use_image(image_path, color_similarity_threshold=5):\n",
    "    \"\"\"\n",
    "    Evaluate a standardized land use image based on diversity, density, and design.\n",
    "    Modified version with better diagnostics for block detection.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: Path to the standardized land use image\n",
    "    - color_similarity_threshold: Threshold for color matching (use small values for standardized images)\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with evaluation metrics\n",
    "    - List of color masks\n",
    "    - Building mask\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width, _ = img.shape\n",
    "    total_pixels = height * width\n",
    "\n",
    "    # 1. Diversity: Extract unique colors from standardized image\n",
    "    unique_colors, counts = np.unique(img.reshape(-1, 3), axis=0, return_counts=True)\n",
    "    \n",
    "    # Filter colors based on significance\n",
    "    significant_colors = []\n",
    "    significant_counts = []\n",
    "    \n",
    "    # Lower threshold for standardized images (0.1% of total pixels)\n",
    "    min_pixel_threshold = total_pixels * 0.001\n",
    "    \n",
    "    for color, count in zip(unique_colors, counts):\n",
    "        # Skip white/very light colors (likely roads or background)\n",
    "        if np.mean(color) > 240:\n",
    "            continue\n",
    "        \n",
    "        # Only keep colors that appear in a meaningful portion of the image\n",
    "        if count >= min_pixel_threshold:\n",
    "            significant_colors.append(color)\n",
    "            significant_counts.append(count)\n",
    "    \n",
    "    # Sort by count (descending) for consistency\n",
    "    sorted_indices = np.argsort(significant_counts)[::-1]\n",
    "    significant_colors = [significant_colors[i] for i in sorted_indices]\n",
    "    significant_counts = [significant_counts[i] for i in sorted_indices]\n",
    "    \n",
    "    # Debug information\n",
    "    print(f\"Found {len(unique_colors)} total unique colors\")\n",
    "    print(f\"Found {len(significant_colors)} significant colors after filtering\")\n",
    "    \n",
    "    # Print top colors for debugging\n",
    "    print(\"\\nTop significant colors:\")\n",
    "    for i, (color, count) in enumerate(zip(significant_colors[:10], significant_counts[:10])):\n",
    "        print(f\"  Color {i}: RGB{tuple(color)} - {count} pixels ({count/total_pixels*100:.2f}%)\")\n",
    "\n",
    "    # 2. Density: Calculate building coverage\n",
    "    building_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Define color ranges for typical buildings in land use maps\n",
    "    # Pink/Red buildings (residential, commercial)\n",
    "    pink_lower = np.array([180, 100, 100])\n",
    "    pink_upper = np.array([255, 180, 180])\n",
    "    pink_mask = np.all((img >= pink_lower) & (img <= pink_upper), axis=2)\n",
    "\n",
    "    # Yellow/Orange buildings (schools, hospitals, etc.)\n",
    "    yellow_lower = np.array([180, 150, 50])\n",
    "    yellow_upper = np.array([255, 230, 150])\n",
    "    yellow_mask = np.all((img >= yellow_lower) & (img <= yellow_upper), axis=2)\n",
    "\n",
    "    # Gray buildings (industrial)\n",
    "    gray_lower = np.array([150, 150, 150])\n",
    "    gray_upper = np.array([200, 200, 200])\n",
    "    gray_mask = np.all((img >= gray_lower) & (img <= gray_upper), axis=2)\n",
    "\n",
    "    # Combine all building masks\n",
    "    building_mask = np.logical_or(np.logical_or(pink_mask, yellow_mask), gray_mask).astype(np.uint8)\n",
    "    building_pixel_count = np.sum(building_mask)\n",
    "    building_coverage_ratio = building_pixel_count / total_pixels\n",
    "\n",
    "    # Store building colors for visualization\n",
    "    building_colors = []\n",
    "    for color in significant_colors:\n",
    "        r, g, b = color\n",
    "        # Check if color is in building range\n",
    "        if (r >= 180 and 100 <= g <= 180 and 100 <= b <= 180) or \\\n",
    "           (r >= 180 and 150 <= g <= 230 and 50 <= b <= 150) or \\\n",
    "           (150 <= r <= 200 and 150 <= g <= 200 and 150 <= b <= 200):\n",
    "            building_colors.append(color)\n",
    "\n",
    "    # 3. Design: Calculate average block size\n",
    "    avg_block_sizes = []\n",
    "    color_masks = []\n",
    "    block_counts = []\n",
    "    all_block_info = []  # Store detailed info for each color\n",
    "    \n",
    "    # Lower minimum block size threshold for better detection\n",
    "    min_block_size_threshold = 200  # Reduced from 600 to 50 pixels\n",
    "    \n",
    "    #print(f\"\\nAnalyzing blocks (min size threshold: {min_block_size_threshold} pixels):\")\n",
    "\n",
    "    # Reshape the image once for efficient color matching\n",
    "    reshaped_img = img.reshape(-1, 3)\n",
    "    \n",
    "    for i, color in enumerate(significant_colors):\n",
    "        print(f\"\\nColor {i} (RGB: {color}):\")\n",
    "        \n",
    "        # DIRECT SOLUTION: Use the same method as in np.unique to create masks\n",
    "        # Match pixels exactly using array comparison\n",
    "        color_matches = np.all(reshaped_img == color, axis=1)\n",
    "        # Reshape back to original image dimensions\n",
    "        color_mask = color_matches.reshape(height, width).astype(np.uint8)\n",
    "        \n",
    "        # Store the mask for later use\n",
    "        color_masks.append(color_mask)\n",
    "        \n",
    "        # Check how many pixels match this color\n",
    "        matched_pixels = np.sum(color_mask)\n",
    "        print(f\"  Matched pixels: {matched_pixels} ({matched_pixels/total_pixels*100:.2f}%)\")\n",
    "\n",
    "        # Label connected components\n",
    "        labeled_mask, num_features = ndimage.label(color_mask)\n",
    "        print(f\"  Total connected components: {num_features}\")\n",
    "\n",
    "        if num_features > 0:\n",
    "            # Measure properties of labeled regions\n",
    "            region_sizes = ndimage.sum(color_mask, labeled_mask, range(1, num_features + 1))\n",
    "            \n",
    "            # Show distribution of region sizes\n",
    "            if len(region_sizes) > 0:\n",
    "                print(f\"  Region size stats: min={np.min(region_sizes):.0f}, \"\n",
    "                      f\"max={np.max(region_sizes):.0f}, mean={np.mean(region_sizes):.0f}\")\n",
    "            \n",
    "            # Filter out very small regions (noise)\n",
    "            valid_regions = region_sizes[region_sizes > min_block_size_threshold]\n",
    "            \n",
    "            if len(valid_regions) > 0:\n",
    "                avg_block_size = np.mean(valid_regions)\n",
    "                avg_block_sizes.append(avg_block_size)\n",
    "                block_counts.append(len(valid_regions))\n",
    "                \n",
    "               # print(f\"  Valid blocks (>{min_block_size_threshold}px): {len(valid_regions)}\")\n",
    "               # print(f\"  Average block size: {avg_block_size:.2f} pixels\")\n",
    "                \n",
    "                # Store detailed info\n",
    "                all_block_info.append({\n",
    "                    'color': color.tolist(),\n",
    "                    'valid_blocks': len(valid_regions),\n",
    "                    'avg_size': avg_block_size,\n",
    "                    'total_components': num_features\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  No blocks larger than threshold ({min_block_size_threshold}px)\")\n",
    "                print(f\"  Largest block was: {np.max(region_sizes):.0f} pixels\")\n",
    "                \n",
    "                # Try with a lower threshold to see if blocks exist\n",
    "                test_threshold = 10\n",
    "                test_valid = region_sizes[region_sizes > test_threshold]\n",
    "                if len(test_valid) > 0:\n",
    "                    print(f\"  With threshold={test_threshold}px: {len(test_valid)} blocks found\")\n",
    "\n",
    "    # Calculate overall average block size\n",
    "    if avg_block_sizes:\n",
    "        overall_avg_block_size = np.mean(avg_block_sizes)\n",
    "       # print(f\"\\nOverall average block size: {overall_avg_block_size:.2f} pixels\")\n",
    "       # print(f\"Total colors with valid blocks: {len(avg_block_sizes)}\")\n",
    "    else:\n",
    "        overall_avg_block_size = 0\n",
    "        print(\"\\nWARNING: No valid blocks found! Average block size is 0.\")\n",
    "        print(\"Consider:\")\n",
    "        print(\"1. Reducing min_block_size_threshold\")\n",
    "        print(\"2. Increasing color_similarity_threshold\")\n",
    "        print(\"3. Checking if colors are forming connected regions\")\n",
    "\n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        \"diversity\": {\n",
    "            \"unique_land_use_categories\": len(significant_colors),\n",
    "            \"colors\": [color.tolist() for color in significant_colors],\n",
    "            \"land_use_distribution\": {f\"color_{i}\": count/sum(significant_counts)\n",
    "                                     for i, count in enumerate(significant_counts)}\n",
    "        },\n",
    "        \"density\": {\n",
    "            \"building_coverage_ratio\": building_coverage_ratio,\n",
    "            \"building_colors\": [color.tolist() for color in building_colors],\n",
    "            \"building_pixel_count\": int(building_pixel_count),\n",
    "            \"total_pixels\": total_pixels\n",
    "        },\n",
    "        \"design\": {\n",
    "            \"average_block_size\": overall_avg_block_size,\n",
    "            \"block_size_by_category\": {f\"color_{i}\": size for i, size in enumerate(avg_block_sizes)},\n",
    "            \"block_counts\": {f\"color_{i}\": count for i, count in enumerate(block_counts)},\n",
    "            \"min_block_size_threshold\": min_block_size_threshold,\n",
    "            \"detailed_block_info\": all_block_info\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results, color_masks, building_mask\n",
    "\n",
    "def visualize_results(image_path, results, color_masks, building_mask, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the evaluation results\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: Path to the original image\n",
    "    - results: Evaluation results dictionary\n",
    "    - color_masks: List of color masks\n",
    "    - building_mask: Building mask\n",
    "    - save_path: Path to save the visualization (if None, display instead)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image for visualization: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Land Use Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Building mask\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(building_mask, cmap='binary')\n",
    "    plt.title(f\"Building Coverage: {results['density']['building_coverage_ratio']:.2f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Color segmentation\n",
    "    plt.subplot(2, 3, 3)\n",
    "    segmentation = np.zeros_like(img)\n",
    "    colors = results[\"diversity\"][\"colors\"]\n",
    "\n",
    "    for i, mask in enumerate(color_masks):\n",
    "        if i < len(colors):\n",
    "            color = colors[i]\n",
    "            for c in range(3):\n",
    "                segmentation[:,:,c] = np.where(mask == 1, color[c], segmentation[:,:,c])\n",
    "\n",
    "    plt.imshow(segmentation)\n",
    "    plt.title(f\"Color Segmentation: {len(colors)} categories\")\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "# Diversity visualization\n",
    "    plt.subplot(2, 3, 4)\n",
    "    categories = results[\"diversity\"][\"unique_land_use_categories\"]\n",
    "    distribution = list(results[\"diversity\"][\"land_use_distribution\"].values())\n",
    "\n",
    "    bars = plt.bar(range(len(distribution)), distribution)\n",
    "    plt.title(f\"Land Use Diversity: {categories} categories\")\n",
    "    plt.xlabel(\"Land Use Category\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "\n",
    "    # Color the bars with their corresponding land use colors\n",
    "    for i, bar in enumerate(bars):\n",
    "        if i < len(colors):\n",
    "            bar.set_color([c/255 for c in colors[i]])\n",
    "\n",
    "    # Density visualization\n",
    "    plt.subplot(2, 3, 5)\n",
    "    building_ratio = results[\"density\"][\"building_coverage_ratio\"]\n",
    "\n",
    "    plt.pie([building_ratio, 1-building_ratio],\n",
    "            labels=[\"Buildings\", \"Non-buildings\"],\n",
    "            autopct='%1.1f%%')\n",
    "    plt.title(f\"Building Density: {building_ratio:.2f}\")\n",
    "\n",
    "    # Design visualization\n",
    "    plt.subplot(2, 3, 6)\n",
    "    avg_block_size = results[\"design\"][\"average_block_size\"]\n",
    "    block_sizes = list(results[\"design\"][\"block_size_by_category\"].values())\n",
    "\n",
    "    if block_sizes:\n",
    "        bars = plt.bar(range(len(block_sizes)), block_sizes)\n",
    "        plt.title(f\"Design: Avg Block Size {avg_block_size:.2f}\")\n",
    "        plt.xlabel(\"Land Use Category\")\n",
    "        plt.ylabel(\"Average Block Size (pixels)\")\n",
    "\n",
    "        # Color the bars with their corresponding land use colors\n",
    "        for i, bar in enumerate(bars):\n",
    "            if i < len(colors):\n",
    "                bar.set_color([c/255 for c in colors[i]])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return segmentation\n",
    "\n",
    "def process_image_folder(folder_path, output_folder=None, extension=\"*.png\"):\n",
    "    \"\"\"\n",
    "    Process all images in a folder\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path: Path to the folder containing images\n",
    "    - output_folder: Path to save visualization results (if None, display instead)\n",
    "    - extension: File extension to look for\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with results for all images\n",
    "    \"\"\"\n",
    "    # Create output folder if specified\n",
    "    if output_folder and not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        os.makedirs(os.path.join(output_folder, \"visualizations\"), exist_ok=True)\n",
    "    \n",
    "    # Get all image files in the folder\n",
    "    image_files = glob.glob(os.path.join(folder_path, extension))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No {extension} files found in {folder_path}\")\n",
    "        return {}\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    # Process each image\n",
    "    for img_path in image_files:\n",
    "        print(f\"Processing {img_path}...\")\n",
    "        \n",
    "        # Get base filename for output\n",
    "        base_name = os.path.basename(img_path)\n",
    "        file_name = os.path.splitext(base_name)[0]\n",
    "        \n",
    "        # Evaluate the image\n",
    "        results, color_masks, building_mask = evaluate_land_use_image(img_path)\n",
    "        \n",
    "        if results is None:\n",
    "            print(f\"Skipping {img_path} due to processing error\")\n",
    "            continue\n",
    "            \n",
    "        all_results[base_name] = results\n",
    "        \n",
    "        # Visualize and save or display results\n",
    "        if output_folder:\n",
    "            vis_path = os.path.join(output_folder, \"visualizations\", f\"{file_name}_analysis.png\")\n",
    "            visualize_results(img_path, results, color_masks, building_mask, save_path=vis_path)\n",
    "        else:\n",
    "            print(f\"\\nResults for {base_name}:\")\n",
    "            print(f\"Diversity: {results['diversity']['unique_land_use_categories']} unique land use categories\")\n",
    "            print(f\"Density: Building coverage ratio = {results['density']['building_coverage_ratio']:.2f}\")\n",
    "            print(f\"Design: Average block size = {results['design']['average_block_size']:.2f} pixels\")\n",
    "            \n",
    "            # Show visualization\n",
    "            visualize_results(img_path, results, color_masks, building_mask)\n",
    "    \n",
    "    # Save all results to a file if output folder is specified\n",
    "    if output_folder:\n",
    "        import json\n",
    "        with open(os.path.join(output_folder, \"all_results.json\"), 'w') as f:\n",
    "            json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Process a single image\n",
    "    # image_path = \"./extracted_osmcubic/osm_map_3.png\"\n",
    "    # results, color_masks, building_mask = evaluate_land_use_image(image_path)\n",
    "    # print(\"Evaluation Results:\")\n",
    "    # print(f\"Diversity: {results['diversity']['unique_land_use_categories']} unique land use categories\")\n",
    "    # print(f\"Density: Building coverage ratio = {results['density']['building_coverage_ratio']:.2f}\")\n",
    "    # print(f\"Design: Average block size = {results['design']['average_block_size']:.2f} pixels\")\n",
    "    # visualize_results(image_path, results, color_masks, building_mask)\n",
    "    \n",
    "    # Process all images in a folder\n",
    "    folder_path = \"./standardized_the1m6\"\n",
    "    output_folder = \"./anastandardized_the1m6\"\n",
    "    \n",
    "    # Supports PNG files by default, change extension for other file types\n",
    "    all_results = process_image_folder(folder_path, output_folder, extension=\"*.png\")\n",
    "    \n",
    "    print(f\"Processed {len(all_results)} images. Results saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a4b93a-aeb4-4c39-905e-01f201189993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已平衡两个文件夹的图片数量至 1999 张\n",
      "提取真实图像特征...\n",
      "提取生成图像特征...\n",
      "计算FID值...\n",
      "FID值: 58.58846480018747\n",
      "计算耗时: 36.31 秒\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.files = [f for f in os.listdir(path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.path, self.files[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "def extract_features(dataloader, model, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            feat = model(batch)\n",
    "            features.append(feat.cpu().numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    return features\n",
    "\n",
    "def calculate_fid(real_features, gen_features):\n",
    "   \n",
    "    mu1 = np.mean(real_features, axis=0)\n",
    "    mu2 = np.mean(gen_features, axis=0)\n",
    "    \n",
    "  \n",
    "    sigma1 = np.cov(real_features, rowvar=False)\n",
    "    sigma2 = np.cov(gen_features, rowvar=False)\n",
    "    \n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "   \n",
    "    eps = 1e-6\n",
    "    sigma1 = sigma1 + np.eye(sigma1.shape[0]) * eps\n",
    "    sigma2 = sigma2 + np.eye(sigma2.shape[0]) * eps\n",
    "    \n",
    "   \n",
    "    covmean_sq = sigma1.dot(sigma2)\n",
    "    \n",
    "    \n",
    "    if np.iscomplexobj(covmean_sq):\n",
    "        print(\"warning\")\n",
    "        covmean_sq = covmean_sq.real\n",
    "    \n",
    "   \n",
    "    eigvals = np.linalg.eigvals(covmean_sq)\n",
    "    eigvals = np.maximum(eigvals, 0)  \n",
    "    covmean_trace = np.sum(np.sqrt(eigvals))\n",
    "    \n",
    "    \n",
    "    fid = np.sum(diff**2) + np.trace(sigma1) + np.trace(sigma2) - 2 * covmean_trace\n",
    "    \n",
    "    return fid\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    real_merged_dir = \"./merged6_real_images\"\n",
    "    gen_merged_dir = \"./merged66_generated_images\"\n",
    "    \n",
    "    \n",
    "    temp_real_dir = \"./temp_real_images\"\n",
    "    temp_gen_dir = \"./temp_gen_images\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        os.makedirs(temp_real_dir, exist_ok=True)\n",
    "        os.makedirs(temp_gen_dir, exist_ok=True)\n",
    "        \n",
    "        \n",
    "        real_images = [f for f in os.listdir(real_merged_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        gen_images = [f for f in os.listdir(gen_merged_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        \n",
    "        min_count = min(len(real_images), len(gen_images))\n",
    "        \n",
    "        \n",
    "        selected_real = random.sample(real_images, min_count)\n",
    "        selected_gen = random.sample(gen_images, min_count)\n",
    "        \n",
    "        \n",
    "        for img in selected_real:\n",
    "            shutil.copy(os.path.join(real_merged_dir, img), os.path.join(temp_real_dir, img))\n",
    "        \n",
    "        for img in selected_gen:\n",
    "            shutil.copy(os.path.join(gen_merged_dir, img), os.path.join(temp_gen_dir, img))\n",
    "        \n",
    "        print(f\"已平衡两个文件夹的图片数量至 {min_count} 张\")\n",
    "        \n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Inception\n",
    "        model = models.inception_v3(pretrained=True, transform_input=False)\n",
    "        \n",
    "        model.fc = torch.nn.Identity()\n",
    "        model = model.to(device)\n",
    "        \n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        real_dataset = ImageDataset(temp_real_dir, transform)\n",
    "        gen_dataset = ImageDataset(temp_gen_dir, transform)\n",
    "        \n",
    "        real_loader = DataLoader(real_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "        gen_loader = DataLoader(gen_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        print(\"提取真实图像特征...\")\n",
    "        real_features = extract_features(real_loader, model, device)\n",
    "        print(\"提取生成图像特征...\")\n",
    "        gen_features = extract_features(gen_loader, model, device)\n",
    "        \n",
    "        # FID\n",
    "        print(\"计算FID值...\")\n",
    "        fid_value = calculate_fid(real_features, gen_features)\n",
    "        \n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"FID值: {fid_value}\")\n",
    "        print(f\"计算耗时: {elapsed_time:.2f} 秒\")\n",
    "    \n",
    "    finally:\n",
    "        \n",
    "        if os.path.exists(temp_real_dir):\n",
    "            shutil.rmtree(temp_real_dir)\n",
    "        if os.path.exists(temp_gen_dir):\n",
    "            shutil.rmtree(temp_gen_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
